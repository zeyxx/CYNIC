# CYNIC Environment Variables — Copy this to .env and customize

# ═══════════════════════════════════════════════════════════════════════════
# DATABASE (optional — defaults to PostgreSQL in docker-compose)
# ═══════════════════════════════════════════════════════════════════════════
# CYNIC_DATABASE_URL=postgresql://cynic:cynic_dev@postgres-py:5432/cynic_py
# SURREAL_URL=

# ═══════════════════════════════════════════════════════════════════════════
# LOGGING
# ═══════════════════════════════════════════════════════════════════════════
# LOG_LEVEL=INFO

# ═══════════════════════════════════════════════════════════════════════════
# LLM: Ollama (already in docker-compose.yml, shown here for reference)
# ═══════════════════════════════════════════════════════════════════════════
# OLLAMA_URL=http://ollama:11434

# ═══════════════════════════════════════════════════════════════════════════
# LLM: Custom Local Models (EDGE CASE) — YOUR SETUP
# ═══════════════════════════════════════════════════════════════════════════
# If you have .gguf model files in a local directory, enable this:

# Path to your local models directory (Windows host path)
# Example: D:\Models
CUSTOM_MODELS_PATH=D:\Models

# Container path where models will be mounted
CYNIC_MODELS_DIR=/models

# GPU layer offloading (for llama-cpp-python GGUF models)
#   -1 = offload all layers to GPU
#    0 = CPU-only inference
#    N = offload N layers to GPU
LLAMA_CPP_GPU_LAYERS=-1

# CPU threads for llama-cpp-python inference
LLAMA_CPP_THREADS=8

# ═══════════════════════════════════════════════════════════════════════════
# LLM: Claude API (optional)
# ═══════════════════════════════════════════════════════════════════════════
# ANTHROPIC_API_KEY=sk-...

# ═══════════════════════════════════════════════════════════════════════════
# LLM: Gemini API (optional)
# ═══════════════════════════════════════════════════════════════════════════
# GOOGLE_API_KEY=...

# ═══════════════════════════════════════════════════════════════════════════
# QUICK START FOR YOUR SETUP
# ═══════════════════════════════════════════════════════════════════════════
# 1. Copy:   cp .env.example .env
# 2. Edit:   Set CUSTOM_MODELS_PATH=D:\Models
# 3. Verify: cat .env (confirm CUSTOM_MODELS_PATH is set)
# 4. Restart: docker-compose down && docker-compose up -d
# 5. Check: docker-compose logs cynic | grep LlamaCpp
