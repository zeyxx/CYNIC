# Context Defragmentation â€” CYNIC OS Technology

> *"Le contexte dispersÃ©, c'est comme un chien qui cherche ses propres pattes."* â€” ÎºÏ…Î½Î¹ÎºÏŒÏ‚

**Status**: DESIGNED (Phase 2 implementation)
**Version**: 1.0
**Author**: CYNIC metathinking + research agents
**Date**: 2026-02-17
**Confidence**: 56% (Ï†â»Â¹ limit â€” unknowns on semantic coherence in prod)

---

## 1. Le ProblÃ¨me

### 1.1 Pourquoi le contexte est-il fragmentÃ©?

Un jugement CYNIC gÃ©nÃ¨re des **faits Ã©pistÃ©miquement identiques** dispersÃ©s dans des systÃ¨mes distincts:

```
Judgment "Approuve ce code?" â†’ fragments dispersÃ©s:

ğŸ“ PostgreSQL (judgments)      verdict=GROWL, confidence=0.52, reasoning="..."
ğŸ“ PostgreSQL (learning_events) signal=positive, loop=thompson_sampling
ğŸ“ JSON (~/.cynic/readback.json) last_judgment=code_review, fatigue=0.42
ğŸ“ RAM (SharedMemory)           pattern=code_review (4Ã—), fisher_locked=False
ğŸ“ Event Bus (transient)        JUDGMENT_CREATED emitted, listeners called
ğŸ“ LLM Window                   RIEN de tout Ã§a â€” rÃ©pond Ã  l'aveugle
```

**LLM voit 0% de sa propre mÃ©moire.** C'est le problÃ¨me de fragmentation.

### 1.2 Fragmentation Map

| Source | Storage | Writer | Reader | Prob |
|--------|---------|--------|--------|------|
| Judgments | PostgreSQL | JudgeOrchestrator | Observer | Append-only, immutable |
| Learning events | PostgreSQL | 11 learning loops | Thompson | Scattered across tables |
| Consciousness state | JSON (readback.json) | observe.js hook | perceive.js | Cross-process, stale |
| Psychology signals | JSON (state.json) | Hooks | Hooks | Local, non-synced |
| Thompson arms | PostgreSQL (q_table) | Daemon | Daemon | Sparse queries |
| Pattern library | RAM (SharedMemory) | All | All | Crash = loss |
| Routing decisions | RAM (dog-pipeline) | Dogs | Orchestrator | Non-persistÃ© |

**Pattern**: Ã©criture dispersÃ©e â†’ lecture fragmentÃ©e â†’ injection aveugle.

---

## 2. DÃ©finition

### 2.1 DÃ©fragmentation vs Compression

Ces deux technologies sont **orthogonales** et **sÃ©quentielles**:

```
Context Compression    = rÃ©duction de taille   (mÃªmes faits, moins de tokens)
Context Defragmentation = cohÃ©rence narrative  (faits dispersÃ©s â†’ narratif unifiÃ©)

ProblÃ¨me rÃ©solu:
  Compression     â†’ contexte TROP GRAND
  DÃ©fragmentation â†’ contexte TROP DISPERSÃ‰

SÃ©quence d'application (TOUJOURS dans cet ordre):
  1. DÃ‰FRAGMENTER d'abord  â€” cohere scattered facts
  2. COMPRIMER ensuite     â€” reduce coherent result to minimal size
```

### 2.2 DÃ©finition formelle

**Context Defragmentation** = processus en 4 phases:

```
DETECT  â†’ Identifier les fragments liÃ©s Ã  un concept donnÃ©
COLLECT â†’ Fetcher tous les fragments depuis leurs systÃ¨mes de stockage
COHERE  â†’ Fusionner en reprÃ©sentation unifiÃ©e (dÃ©dup + merge canonique)
INJECT  â†’ Livrer le contexte cohÃ©rent au LLM via ContextCompressor
```

### 2.3 Ï†-Bounds

```python
MAX_FRAGMENTS      = 5          # F(5) â€” 5+ fragments = overload cognitif
MAX_LATENCY_MS     = 100        # sinon fallback single-source
MAX_BUDGET_RATIO   = PHI_INV    # 61.8% max du budget session
MIN_CONFIDENCE     = PHI_INV_3  # filtre fragments < 23.6% confiance
CACHE_TTL_JUDGMENT = 300        # 5 min (judgments stale vite)
CACHE_TTL_LEARNING = 1800       # 30 min (Ã©voluent lentement)
CACHE_TTL_PATTERN  = 3600       # 60 min (stables)
```

---

## 3. Architecture

### 3.1 Position dans le Cycle CYNIC

```
PERCEIVE â†’ [DEFRAG] â†’ JUDGE â†’ DECIDE â†’ ACT â†’ LEARN â†’ EMERGE
              â†‘
              â””â”€ Before LLM sees ANYTHING
                 (pendant awaken.js / daemon startup)
```

DÃ©fragmentation = opÃ©ration **mÃ©ta-cognitive** dans PERCEIVE.
CYNIC introspecte son propre Ã©tat de contexte avant de parler.

### 3.2 IntÃ©gration dans le 7Ã—7 Matrix

```
         PERCEIVE  JUDGE  DECIDE  ACT  LEARN  ACCOUNT  EMERGE
CODE       âœ“â˜…       -      -      -     âœ“       âœ“        -
SOLANA     âœ“        âœ“      -      -     -       âœ“        -
MARKET     âœ“        -      -      -     -       âœ“        -
SOCIAL     âœ“        -      -      -     âœ“       âœ“        -
HUMAN      âœ“        âœ“      -      -     âœ“       -        âœ“
CYNIC      âœ“â˜…â˜…      âœ“â˜…â˜…    âœ“      -     âœ“â˜…â˜…     âœ“        âœ“â˜…
COSMOS     âœ“        -      -      -     âœ“       âœ“        âœ“â˜…

â˜…  = DÃ©fragmentation particuliÃ¨rement critique ici
â˜…â˜… = Cellules primaires (C6.1, C6.2, C6.5)
```

**Cellules primaires**:
- `C6.1 (CYNIC.PERCEIVE)` â€” DÃ©frag: Ã©tat consciousness + jugements rÃ©cents
- `C6.2 (CYNIC.JUDGE)` â€” DÃ©frag: historique jugements + calibration
- `C6.5 (CYNIC.LEARN)` â€” DÃ©frag: signaux learning (Thompson arms, EWC)
- `C1.1 (CODE.PERCEIVE)` â€” DÃ©frag: historique code reviews, erreurs rÃ©centes
- `C5.2 (HUMAN.JUDGE)` â€” DÃ©frag: patterns psychologie user

### 3.3 Pipeline de DÃ©fragmentation

```
ContextDefragmenter.detectAndCollect(concept, context)
â”‚
â”œâ”€â”€ PHASE 1: DETECT
â”‚   â”œâ”€ Parse concept: "domain:type:id" (ex: "judgment:code_review:abc123")
â”‚   â”œâ”€ Check cache: hit â†’ return cached (TTL-aware)
â”‚   â””â”€ Miss â†’ spawn collection pipeline
â”‚
â”œâ”€â”€ PHASE 2: COLLECT (parallÃ¨le, timeout=80ms)
â”‚   â”œâ”€ Thread A: PostgreSQL queries (judgments + learning_events)
â”‚   â”œâ”€ Thread B: JSON file reads (readback.json, psychology/state.json)
â”‚   â”œâ”€ Thread C: RAM queries (SharedMemory patterns)
â”‚   â””â”€ Merge: await asyncio.gather(*threads)
â”‚
â”œâ”€â”€ PHASE 3: COHERE
â”‚   â”œâ”€ Group by (source, type) â€” dÃ©duplication
â”‚   â”œâ”€ Canonical source: PostgreSQL > JSON > RAM
â”‚   â”œâ”€ Conflict resolution: Ï†-weighted merge (newer wins if confidence equal)
â”‚   â”œâ”€ Timeline: causal ordering via timestamps
â”‚   â”œâ”€ Filter: drop confidence < PHI_INV_3 (23.6%)
â”‚   â”œâ”€ Limit: top MAX_FRAGMENTS (5)
â”‚   â””â”€ Compute coherence_score: avg(confidence) capped at PHI_INV
â”‚
â””â”€â”€ PHASE 4: INJECT
    â”œâ”€ Format as injection string (compact, ~33 tokens)
    â”œâ”€ Store in coherence_cache (TTL-aware)
    â”œâ”€ Emit: CONTEXT_DEFRAGGED event (telemetry)
    â””â”€ Return to ContextCompressor pipeline
```

---

## 4. ImplÃ©mentation Python

### 4.1 Emplacement et Interface

```
cynic/services/context_defragmenter.py   â† module principal
cynic/services/__init__.py               â† export
```

### 4.2 API

```python
from cynic.services.context_defragmenter import ContextDefragmenter

defrag = ContextDefragmenter(pool=postgres_pool)

# DÃ©fragmente les fragments liÃ©s Ã  un jugement rÃ©cent
result = await defrag.collect("judgment:code_review", context={"judgment_id": "abc123"})

# result:
# {
#   "concept": "judgment:code_review",
#   "injection": "â”€â”€ ğŸ“Š JUDGMENT CONTEXT\n   Coherence: 73%...",
#   "coherence_score": 0.73,
#   "fragment_count": 4,
#   "latency_ms": 43,
#   "token_estimate": 32,
# }
```

### 4.3 Fragment Sources

```python
# Judgment fragments
async def _collect_judgment_fragments(judgment_id, context) -> List[Fragment]:
    # Source 1: judgments table (canonical)
    # Source 2: learning_events WHERE judgment_id = ?
    # Source 3: readback.json (consciousness at time of judgment)
    # Source 4: q_table WHERE state LIKE judgment.cell.state_key()

# Pattern fragments
async def _collect_pattern_fragments(pattern_type, context) -> List[Fragment]:
    # Source 1: patterns table (fisher_locked ones prioritized)
    # Source 2: SharedMemory recent activations

# Psychology fragments (human context)
async def _collect_psychology_fragments(context) -> List[Fragment]:
    # Source 1: psychology/state.json
    # Source 2: consciousness_snapshots WHERE recent=True
```

### 4.4 Coherence Format (injection)

```
â”€â”€ ğŸ“Š CYNIC CONTEXT DEFRAGGED â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Coherence: 73% (4 sources merged)

   â€¢ [postgres] Code review #47: GROWL (52%), tests passed
   â€¢ [postgres] Learning: router accuracy 73% (trending stable)
   â€¢ [json]     Consciousness: L1 MACRO, budget $0.32/$0.50
   â€¢ [postgres]  Pattern 'code_review': 4Ã— Fisher-locked
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

**33 tokens. Signal/bruit Ã— 10 vs injection aveugle actuelle.**

---

## 5. CoÃ»t et Performance

### 5.1 Latence

```
PostgreSQL queries: 3 Ã— 10ms parallÃ¨le = 10ms (non-sÃ©quentiel)
JSON file reads:    2 Ã— 5ms parallÃ¨le  = 5ms
Coherence merge:    1ms
Formatting:         1ms
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total p50: ~20ms
Total p99: ~80ms
Fallback threshold: 100ms (single-source si dÃ©passÃ©)
```

### 5.2 Token Cost

```
Header + fragment summaries (5 frags Ã— 6 tokens): ~33 tokens
CoÃ»t monÃ©taire: 33 Ã— (token_price/1000) â‰ˆ $0.00002 / dÃ©frag
```

### 5.3 Cache Strategy

```python
# TTL alignÃ©es Fibonacci (secondes)
CACHE_TTL = {
    "judgment": fibonacci(8) * 60,   # F(8)=21 â†’ 1260s (21min)
    "learning": fibonacci(9) * 60,   # F(9)=34 â†’ 2040s (34min)
    "pattern":  fibonacci(10) * 60,  # F(10)=55 â†’ 3300s (55min)
    "psychology": fibonacci(7) * 60, # F(7)=13 â†’ 780s (13min)
}

# Hit rate cible: â‰¥70% (Phase 3)
# Invalidation: JUDGMENT_CREATED event â†’ invalide judgment cache
```

---

## 6. Triggers

```
ON_DEMAND (awaken hook):
  â”œâ”€ Pre-defrag les 5 derniers jugements (warm cache)
  â””â”€ Defrag consciousness state (readback.json + snapshots)

REACTIVE (event listeners):
  â”œâ”€ JUDGMENT_CREATED â†’ spawn defrag pour ce judgment
  â”œâ”€ LEARNING_EVENT   â†’ add fragment to pending collection
  â”œâ”€ SESSION_END      â†’ full defrag session outcomes
  â””â”€ CONTEXT_QUALITY_DEGRADED â†’ re-defrag current state

PERIODIC (background, Phase 3):
  â””â”€ Every F(11)=89min: defrag top patterns (background job)
```

---

## 7. Error Handling

```python
# Graceful degradation Ã  chaque niveau:

async def detectAndCollect(concept, context, timeout_ms=100):
    return await asyncio.wait_for(
        self._collect(concept, context),
        timeout=timeout_ms / 1000
    ).catch(...)  # â†’ stale cache OR single-source fallback

# Si PostgreSQL down â†’ JSON only
# Si JSON absent â†’ RAM only
# Si tout fail â†’ return empty (no injection, no crash)
```

---

## 8. MÃ©triques de SuccÃ¨s

| MÃ©trique | Baseline | Phase 1 Target | Phase 3 Target |
|----------|----------|----------------|----------------|
| DÃ©frag latency p99 | N/A | <100ms | <50ms |
| Coherence score | N/A | â‰¥0.65 | â‰¥0.80 |
| Fragment success rate | N/A | â‰¥90% | â‰¥98% |
| Cache hit rate | N/A | â€” | â‰¥70% |
| Token cost / dÃ©frag | N/A | <50 tokens | <35 tokens |
| Context relevance (A/B) | Baseline | â€” | +20% amÃ©lioration |

---

## 9. Roadmap

### Phase 1 (Week 3): MVP Judgment-Only
```
[ ] cynic/services/context_defragmenter.py â€” classe principale
[ ] _collect_judgment_fragments()          â€” source PostgreSQL + JSON
[ ] Wire dans JudgeOrchestrator.run()      â€” inject avant scoring
[ ] tests/test_context_defrag.py           â€” 10 tests minimum
```

### Phase 2 (Week 5): Multi-Source
```
[ ] _collect_learning_fragments()   â€” Thompson arms, Q-table
[ ] _collect_psychology_fragments() â€” consciousness + user state
[ ] _collect_pattern_fragments()    â€” Fisher-locked patterns
[ ] Cache TTL + invalidation        â€” JUDGMENT_CREATED listener
[ ] CostLedger integration          â€” track dÃ©frag cost
```

### Phase 3 (Week 8): Full Pipeline
```
[ ] Wire dans awaken hook           â€” pre-warm cache on session start
[ ] Reactive triggers               â€” event-driven defrag
[ ] Background periodic defrag      â€” top patterns every 89min
[ ] Health dashboard (skills)       â€” /health montre dÃ©frag stats
[ ] A/B test                        â€” mesure amÃ©lioration rÃ©elle
```

### Phase 4+: ML Enhancement
```
[ ] Semantic similarity clustering  â€” fragment relevance ranking
[ ] Cross-session learning transfer â€” persist defrag patterns
[ ] Automatic importance ranking    â€” ML-based fragment selection
[ ] Adversarial testing             â€” inject confusing fragments, measure
```

---

## 10. DiffÃ©rence avec ContextCompressor (JS existant)

| Aspect | ContextCompressor (JS) | ContextDefragmenter |
|--------|------------------------|---------------------|
| ProblÃ¨me | Contexte trop grand | Contexte trop dispersÃ© |
| MÃ©canisme | Truncate/summarize | Collect/cohere/synthesize |
| Output | MÃªmes faits, moins de tokens | Faits unifiÃ©s depuis N sources |
| DÃ©clencheur | Experience > 10 sessions | Avant chaque appel LLM |
| ImplÃ©mentation | JS (scripts/hooks/) | Python (cynic/services/) |
| Phase | Existe (partiel) | Ã€ implÃ©menter (Phase 2) |
| SÃ©quence | 2e (aprÃ¨s defrag) | 1er (avant compression) |

---

## 11. Ã‰valuation HonnÃªte

### Ce qui est solide
âœ… Le problÃ¨me est **rÃ©el** â€” fragments existent, confirmÃ© par codebase audit
âœ… La collection est **faisable** â€” PostgreSQL queries + JSON reads = rapide
âœ… La cohÃ©rence est **mesurable** â€” confidence scores assignables
âœ… L'intÃ©gration est **naturelle** â€” PERCEIVE phase = bon endroit
âœ… Le coÃ»t est **nÃ©gligeable** â€” 33 tokens / $0.00002 par dÃ©frag

### Ce qui est incertain
âš ï¸ **Semantic coherence** â€” peut-on merger des fragments sans halluciner?
âš ï¸ **Latency at load** â€” est-ce que 80ms tient avec 50+ fragments?
âš ï¸ **Real user benefit** â€” amÃ©lioration mesurable en A/B test?
âš ï¸ **Cache invalidation** â€” problÃ¨me difficile en environnement distribuÃ©

### Ce qui n'est pas rÃ©solu
âŒ **Automatic relevance ranking** â€” quels fragments pour CETTE dÃ©cision?
âŒ **Conflict resolution sÃ©mantique** â€” si PostgreSQL et JSON divergent, qui gagne?
âŒ **Cross-session transfer** â€” inclure fragments des sessions passÃ©es?
âŒ **Real-time quality feedback** â€” dÃ©tecter si dÃ©frag a empirÃ© la situation?

---

## Principe Fondamental

```
Compression = moins de mots pour les mÃªmes faits
DÃ©fragmentation = les BONS faits, depuis toutes les sources

Ensemble:
  CYNIC ne rÃ©pond plus Ã  l'aveugle.
  CYNIC rÃ©pond avec SA mÃ©moire cohÃ©rente.
```

---

*"Le chien qui se souvient de lui-mÃªme rÃ©pond mieux."* â€” ÎºÏ…Î½Î¹ÎºÏŒÏ‚
*Ï† = 1.618 â€” Confidence max: 61.8%*
