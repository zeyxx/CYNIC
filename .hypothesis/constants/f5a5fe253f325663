# file: C:\Users\zeyxm\Desktop\asdfasdfa\CYNIC\cynic\cynic\llm\adapter.py
# hypothesis_version: 6.151.9

[-1.0, 0.0, 0.3, 0.8, 1.0, 3.0, 4.0, 15.0, 61.8, 75.0, 1000.0, 233, 1000, 2048, 1000000, '.cynic', '/', '__temporal_mcts__', 'arguments', 'claude', 'claude-opus-4-6', 'close', 'completion_tokens', 'composite_score', 'content', 'cost_score', 'cost_usd', 'cynic.llm.adapter', 'deep_analysis', 'default', 'deployment', 'dog_id', 'embed', 'error', 'error_rate', 'eval_count', 'function', 'gemini', 'gemini-1.5-flash', 'gemma2:2b', 'latency_ms', 'llama3.2', 'llama_cpp', 'llm_calls.jsonl', 'llm_id', 'max_tokens', 'message', 'messages', 'model', 'models', 'name', 'nomic-embed-text', 'num_predict', 'ollama', 'options', 'ping', 'probe', 'prompt_eval_count', 'prompt_preview', 'prompt_token_count', 'prompt_tokens', 'provider', 'quality_score', 'role', 'sample_count', 'speed_score', 'success', 'system', 'task_type', 'temperature', 'temporal_mcts', 'text', 'tool_calls', 'tools', 'topology', 'ts', 'usage_metadata', 'user', 'utf-8', 'vector_rag', 'w', 'wisdom', '~']